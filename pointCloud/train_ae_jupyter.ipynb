{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import torch.utils.tensorboard\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from pointCloud.utils.dataset import *\n",
    "from pointCloud.utils.misc import *\n",
    "from pointCloud.utils.data import *\n",
    "from pointCloud.utils.transform import *\n",
    "from pointCloud.models.autoencoder import *\n",
    "from pointCloud.evaluation import EMD_CD"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-18T16:23:12.754733Z",
     "end_time": "2023-06-18T16:23:14.182629Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "# Model arguments\n",
    "parser.add_argument('--latent_dim', type=int, default=256)\n",
    "parser.add_argument('--num_steps', type=int, default=200)\n",
    "parser.add_argument('--beta_1', type=float, default=1e-4)\n",
    "parser.add_argument('--beta_T', type=float, default=0.05)\n",
    "parser.add_argument('--sched_mode', type=str, default='linear')\n",
    "parser.add_argument('--flexibility', type=float, default=0.0)\n",
    "parser.add_argument('--residual', type=eval, default=True, choices=[True, False])\n",
    "# parser.add_argument('--resume', type=str, default=None)\n",
    "\n",
    "parser.add_argument('--resume', type=str, default='D:\\PycharmProjects\\Replay_continual_learning_2\\pointCloud\\logs_ae\\AE_2023_06_17__17_18_04\\ckpt_0.000548_1000.pt')\n",
    "\n",
    "\n",
    "# Datasets and loaders\n",
    "parser.add_argument('--dataset_path', type=str, default='./data/shapenet.hdf5')\n",
    "parser.add_argument('--categories', type=str_list, default=['airplane'])\n",
    "parser.add_argument('--scale_mode', type=str, default='shape_unit')\n",
    "parser.add_argument('--train_batch_size', type=int, default=128)\n",
    "parser.add_argument('--val_batch_size', type=int, default=32)\n",
    "parser.add_argument('--rotate', type=eval, default=False, choices=[True, False])\n",
    "\n",
    "# Optimizer and scheduler\n",
    "parser.add_argument('--lr', type=float, default=1e-3)\n",
    "parser.add_argument('--weight_decay', type=float, default=0)\n",
    "parser.add_argument('--max_grad_norm', type=float, default=10)\n",
    "parser.add_argument('--end_lr', type=float, default=1e-4)\n",
    "parser.add_argument('--sched_start_epoch', type=int, default=150*THOUSAND)\n",
    "parser.add_argument('--sched_end_epoch', type=int, default=300*THOUSAND)\n",
    "\n",
    "# Training\n",
    "parser.add_argument('--seed', type=int, default=2020)\n",
    "parser.add_argument('--logging', type=eval, default=True, choices=[True, False])\n",
    "parser.add_argument('--log_root', type=str, default='./logs_ae')\n",
    "parser.add_argument('--device', type=str, default='cuda')\n",
    "parser.add_argument('--max_iters', type=int, default=float('inf'))\n",
    "parser.add_argument('--val_freq', type=float, default=1000)\n",
    "parser.add_argument('--tag', type=str, default=None)\n",
    "parser.add_argument('--num_val_batches', type=int, default=-1)\n",
    "parser.add_argument('--num_inspect_batches', type=int, default=1)\n",
    "parser.add_argument('--num_inspect_pointclouds', type=int, default=4)\n",
    "args = parser.parse_args([])\n",
    "seed_all(args.seed)  # 将种子值应用于随机数生成器，确保在每次运行代码时使用相同的种子值，从而使结果可复现"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-18T16:23:14.186523Z",
     "end_time": "2023-06-18T16:23:14.196287Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-06-17 17:32:12,353::train::INFO] Namespace(latent_dim=256, num_steps=200, beta_1=0.0001, beta_T=0.05, sched_mode='linear', flexibility=0.0, residual=True, resume='D:\\\\PycharmProjects\\\\Replay_continual_learning_2\\\\pointCloud\\\\logs_ae\\\\AE_2023_06_17__17_18_04\\\\ckpt_0.000548_1000.pt', dataset_path='./data/shapenet.hdf5', categories=['airplane'], scale_mode='shape_unit', train_batch_size=128, val_batch_size=32, rotate=False, lr=0.001, weight_decay=0, max_grad_norm=10, end_lr=0.0001, sched_start_epoch=150000, sched_end_epoch=300000, seed=2020, logging=True, log_root='./logs_ae', device='cuda', max_iters=inf, val_freq=1000, tag=None, num_val_batches=-1, num_inspect_batches=1, num_inspect_pointclouds=4)\n",
      "[2023-06-17 17:32:12,353::train::INFO] Namespace(latent_dim=256, num_steps=200, beta_1=0.0001, beta_T=0.05, sched_mode='linear', flexibility=0.0, residual=True, resume='D:\\\\PycharmProjects\\\\Replay_continual_learning_2\\\\pointCloud\\\\logs_ae\\\\AE_2023_06_17__17_18_04\\\\ckpt_0.000548_1000.pt', dataset_path='./data/shapenet.hdf5', categories=['airplane'], scale_mode='shape_unit', train_batch_size=128, val_batch_size=32, rotate=False, lr=0.001, weight_decay=0, max_grad_norm=10, end_lr=0.0001, sched_start_epoch=150000, sched_end_epoch=300000, seed=2020, logging=True, log_root='./logs_ae', device='cuda', max_iters=inf, val_freq=1000, tag=None, num_val_batches=-1, num_inspect_batches=1, num_inspect_pointclouds=4)\n"
     ]
    }
   ],
   "source": [
    "# Logging\n",
    "# 如果参数中设置了 logging 为 True，则进行日志记录\n",
    "if args.logging:\n",
    "    # 获取一个新的日志目录路径\n",
    "    log_dir = get_new_log_dir(args.log_root, prefix='AE_', postfix='_' + args.tag if args.tag is not None else '')\n",
    "    # 获取一个名为 'train' 的日志记录器，并指定保存目录为 log_dir\n",
    "    logger = get_logger('train', log_dir)\n",
    "    # 创建一个 TensorBoard 的 SummaryWriter 对象，用于将日志写入 TensorBoard\n",
    "    writer = torch.utils.tensorboard.SummaryWriter(log_dir)\n",
    "    # 创建一个 CheckpointManager 对象，用于管理保存检查点文件\n",
    "    ckpt_mgr = CheckpointManager(log_dir)\n",
    "else:\n",
    "    logger = get_logger('train', None)\n",
    "    writer = BlackHole()\n",
    "    ckpt_mgr = BlackHole()\n",
    "\n",
    "# 将参数 args 写入日志文件\n",
    "logger.info(args)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T17:32:12.348444Z",
     "end_time": "2023-06-17T17:32:12.362664Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-06-17 17:32:12,575::train::INFO] Transform: None\n",
      "[2023-06-17 17:32:12,575::train::INFO] Transform: None\n",
      "[2023-06-17 17:32:12,577::train::INFO] Loading datasets...\n",
      "[2023-06-17 17:32:12,577::train::INFO] Loading datasets...\n"
     ]
    }
   ],
   "source": [
    "# Datasets and loaders\n",
    "transform = None\n",
    "if args.rotate:\n",
    "    transform = RandomRotate(180, ['pointcloud'], axis=1)  # 默认不进行旋转\n",
    "logger.info('Transform: %s' % repr(transform))  # 打印数据变换对象的信息\n",
    "logger.info('Loading datasets...')\n",
    "\n",
    "# 加载训练数据集\n",
    "\"\"\"\n",
    "scale_mode有以下三种方式，默认为shape_unit\n",
    "\"shape_unit\"：将每个点云缩放到单位尺寸范围内（[-0.5, 0.5] 或 [0, 1]），使其尺寸归一化。\n",
    "\"shape_sphere\"：将每个点云缩放到单位球体内，使其尺寸和形状归一化。\n",
    "\"shape_sphere_uniform_scale\"：与 \"shape_sphere\" 相同，但会保持原始点云的长宽高比例。\n",
    "\"\"\"\n",
    "\n",
    "train_dset = ShapeNetCore(\n",
    "    path=args.dataset_path,\n",
    "    cates=args.categories,\n",
    "    split='train',\n",
    "    scale_mode=args.scale_mode,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "# 加载验证数据集\n",
    "val_dset = ShapeNetCore(\n",
    "    path=args.dataset_path,\n",
    "    cates=args.categories,\n",
    "    split='val',\n",
    "    scale_mode=args.scale_mode,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "# 创建训练数据集的迭代器，通过使用 get_data_iterator 函数，可以在训练过程中使用 DataLoader 对象进行数据的无限循环迭代。这在训练过程中非常有用，因为它可以保证每个训练样本都能够被使用到，并且在迭代到最后一个样本后能够重新开始迭代，形成一个无限的数据流。\n",
    "train_iter = get_data_iterator(DataLoader(\n",
    "    train_dset,\n",
    "    batch_size=args.train_batch_size,\n",
    "    num_workers=0,\n",
    "))\n",
    "\n",
    "# 创建验证数据集的数据加载器\n",
    "val_loader = DataLoader(val_dset, batch_size=args.val_batch_size, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T17:32:12.576429Z",
     "end_time": "2023-06-17T17:32:13.130717Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T17:32:13.162619Z",
     "end_time": "2023-06-17T17:32:13.167435Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T17:32:13.969181Z",
     "end_time": "2023-06-17T17:32:13.973629Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-06-17 17:32:15,618::train::INFO] Building model...\n",
      "[2023-06-17 17:32:15,618::train::INFO] Building model...\n",
      "[2023-06-17 17:32:15,620::train::INFO] Resuming from checkpoint...\n",
      "[2023-06-17 17:32:15,620::train::INFO] Resuming from checkpoint...\n",
      "[2023-06-17 17:32:17,882::train::INFO] AutoEncoder(\n",
      "  (encoder): PointNetEncoder(\n",
      "    (conv1): Conv1d(3, 128, kernel_size=(1,), stride=(1,))\n",
      "    (conv2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
      "    (conv3): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
      "    (conv4): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "    (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc1_m): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (fc2_m): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (fc3_m): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (fc_bn1_m): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc_bn2_m): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc1_v): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (fc2_v): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (fc3_v): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (fc_bn1_v): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc_bn2_v): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (diffusion): DiffusionPoint(\n",
      "    (net): PointwiseNet(\n",
      "      (layers): ModuleList(\n",
      "        (0): ConcatSquashLinear(\n",
      "          (_layer): Linear(in_features=3, out_features=128, bias=True)\n",
      "          (_hyper_bias): Linear(in_features=259, out_features=128, bias=False)\n",
      "          (_hyper_gate): Linear(in_features=259, out_features=128, bias=True)\n",
      "        )\n",
      "        (1): ConcatSquashLinear(\n",
      "          (_layer): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (_hyper_bias): Linear(in_features=259, out_features=256, bias=False)\n",
      "          (_hyper_gate): Linear(in_features=259, out_features=256, bias=True)\n",
      "        )\n",
      "        (2): ConcatSquashLinear(\n",
      "          (_layer): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (_hyper_bias): Linear(in_features=259, out_features=512, bias=False)\n",
      "          (_hyper_gate): Linear(in_features=259, out_features=512, bias=True)\n",
      "        )\n",
      "        (3): ConcatSquashLinear(\n",
      "          (_layer): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (_hyper_bias): Linear(in_features=259, out_features=256, bias=False)\n",
      "          (_hyper_gate): Linear(in_features=259, out_features=256, bias=True)\n",
      "        )\n",
      "        (4): ConcatSquashLinear(\n",
      "          (_layer): Linear(in_features=256, out_features=128, bias=True)\n",
      "          (_hyper_bias): Linear(in_features=259, out_features=128, bias=False)\n",
      "          (_hyper_gate): Linear(in_features=259, out_features=128, bias=True)\n",
      "        )\n",
      "        (5): ConcatSquashLinear(\n",
      "          (_layer): Linear(in_features=128, out_features=3, bias=True)\n",
      "          (_hyper_bias): Linear(in_features=259, out_features=3, bias=False)\n",
      "          (_hyper_gate): Linear(in_features=259, out_features=3, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (var_sched): VarianceSchedule()\n",
      "  )\n",
      ")\n",
      "[2023-06-17 17:32:17,882::train::INFO] AutoEncoder(\n",
      "  (encoder): PointNetEncoder(\n",
      "    (conv1): Conv1d(3, 128, kernel_size=(1,), stride=(1,))\n",
      "    (conv2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
      "    (conv3): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
      "    (conv4): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "    (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc1_m): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (fc2_m): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (fc3_m): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (fc_bn1_m): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc_bn2_m): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc1_v): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (fc2_v): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (fc3_v): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (fc_bn1_v): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc_bn2_v): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (diffusion): DiffusionPoint(\n",
      "    (net): PointwiseNet(\n",
      "      (layers): ModuleList(\n",
      "        (0): ConcatSquashLinear(\n",
      "          (_layer): Linear(in_features=3, out_features=128, bias=True)\n",
      "          (_hyper_bias): Linear(in_features=259, out_features=128, bias=False)\n",
      "          (_hyper_gate): Linear(in_features=259, out_features=128, bias=True)\n",
      "        )\n",
      "        (1): ConcatSquashLinear(\n",
      "          (_layer): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (_hyper_bias): Linear(in_features=259, out_features=256, bias=False)\n",
      "          (_hyper_gate): Linear(in_features=259, out_features=256, bias=True)\n",
      "        )\n",
      "        (2): ConcatSquashLinear(\n",
      "          (_layer): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (_hyper_bias): Linear(in_features=259, out_features=512, bias=False)\n",
      "          (_hyper_gate): Linear(in_features=259, out_features=512, bias=True)\n",
      "        )\n",
      "        (3): ConcatSquashLinear(\n",
      "          (_layer): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (_hyper_bias): Linear(in_features=259, out_features=256, bias=False)\n",
      "          (_hyper_gate): Linear(in_features=259, out_features=256, bias=True)\n",
      "        )\n",
      "        (4): ConcatSquashLinear(\n",
      "          (_layer): Linear(in_features=256, out_features=128, bias=True)\n",
      "          (_hyper_bias): Linear(in_features=259, out_features=128, bias=False)\n",
      "          (_hyper_gate): Linear(in_features=259, out_features=128, bias=True)\n",
      "        )\n",
      "        (5): ConcatSquashLinear(\n",
      "          (_layer): Linear(in_features=128, out_features=3, bias=True)\n",
      "          (_hyper_bias): Linear(in_features=259, out_features=3, bias=False)\n",
      "          (_hyper_gate): Linear(in_features=259, out_features=3, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (var_sched): VarianceSchedule()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "logger.info('Building model...')\n",
    "# 尝试加载checkpoint\n",
    "if args.resume is not None:\n",
    "    logger.info('Resuming from checkpoint...')\n",
    "    ckpt = torch.load(args.resume)\n",
    "    model = AutoEncoder(ckpt['args']).to(args.device)\n",
    "    model.load_state_dict(ckpt['state_dict'])\n",
    "else:\n",
    "    # 创建新的模型\n",
    "    # 根据 args 中的配置信息创建新的 AutoEncoder 模型对象\n",
    "    model = AutoEncoder(args).to(args.device)\n",
    "logger.info(repr(model))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T17:32:15.621664Z",
     "end_time": "2023-06-17T17:32:17.891314Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=args.lr,\n",
    "                             weight_decay=args.weight_decay\n",
    "                             )\n",
    "# 创建线性学习率调度器\n",
    "scheduler = get_linear_scheduler(\n",
    "    optimizer,\n",
    "    start_epoch=args.sched_start_epoch,\n",
    "    end_epoch=args.sched_end_epoch,\n",
    "    start_lr=args.lr,\n",
    "    end_lr=args.end_lr\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T17:32:17.890340Z",
     "end_time": "2023-06-17T17:32:17.896230Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Train, validate\n",
    "def train(it):\n",
    "    # Load data\n",
    "    batch = next(train_iter)  # 从数据迭代器中获取一个批次的数据\n",
    "    x = batch['pointcloud'].to(args.device)  # 将点云数据转移到指定设备（GPU）上\n",
    "\n",
    "    # Reset grad and model state\n",
    "    optimizer.zero_grad()  # 清零优化器中的梯度\n",
    "    model.train()  # 设置模型为训练模式\n",
    "\n",
    "    # Forward\n",
    "    loss = model.get_loss(x)  # 前向传播计算损失函数  TODO:需要具体看一下get_loss函数\n",
    "\n",
    "    # Backward and optimize\n",
    "    loss.backward()  # 反向传播计算梯度\n",
    "    orig_grad_norm = clip_grad_norm_(model.parameters(), args.max_grad_norm)  # 对梯度进行裁剪，防止梯度爆炸\n",
    "    optimizer.step()  # 对梯度进行裁剪，防止梯度爆炸\n",
    "    scheduler.step()  # 更新学习率\n",
    "\n",
    "    # 写入日志\n",
    "    logger.info('[Train] Iter %04d | Loss %.6f | Grad %.4f ' % (it, loss.item(), orig_grad_norm))\n",
    "    writer.add_scalar('train/loss', loss, it)\n",
    "    writer.add_scalar('train/lr', optimizer.param_groups[0]['lr'], it)\n",
    "    writer.add_scalar('train/grad_norm', orig_grad_norm, it)\n",
    "    writer.flush()  # 更新学习率"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T17:32:19.488789Z",
     "end_time": "2023-06-17T17:32:19.498612Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def validate_loss(it):\n",
    "    # 初始化空的列表用于保存参考点云和重建点云\n",
    "    all_refs = []\n",
    "    all_recons = []\n",
    "    for i, batch in enumerate(tqdm(val_loader, desc='Validate')):\n",
    "        # 如果指定了验证批次数目，并且已经达到指定数目，则退出循环\n",
    "        if args.num_val_batches > 0 and i >= args.num_val_batches:\n",
    "            break\n",
    "        # 将参考点云转移到设备（GPU）\n",
    "        ref = batch['pointcloud'].to(args.device)\n",
    "        shift = batch['shift'].to(args.device)\n",
    "        scale = batch['scale'].to(args.device)\n",
    "        # 禁用梯度计算，设置模型为评估模式\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            # 使用编码器将参考点云编码为潜在空间向量\n",
    "            code = model.encode(ref)\n",
    "            # 使用解码器根据编码向量生成重建点云\n",
    "            recons = model.decode(code, ref.size(1), flexibility=args.flexibility)\n",
    "\n",
    "        # 将参考点云和重建点云进行尺度和偏移还原，并添加到对应的列表中\n",
    "        all_refs.append(ref * scale + shift)\n",
    "        all_recons.append(recons * scale + shift)\n",
    "\n",
    "    # 将列表中的点云拼接起来，得到完整的参考点云和重建点云\n",
    "    all_refs = torch.cat(all_refs, dim=0)\n",
    "    all_recons = torch.cat(all_recons, dim=0)\n",
    "    # 使用拼接后的点云计算指标（如 Chamfer 距离和 Earth Mover's Distance）作为模型在验证集上的损失\n",
    "    metrics = EMD_CD(all_recons, all_refs, batch_size=args.val_batch_size)\n",
    "    cd, emd = metrics['MMD-CD'].item(), metrics['MMD-EMD'].item()\n",
    "\n",
    "    # 将计算得到的损失值记录到日志中，并使用 TensorBoard 进行可视化\n",
    "    logger.info('[Val] Iter %04d | CD %.6f | EMD %.6f  ' % (it, cd, emd))\n",
    "    writer.add_scalar('val/cd', cd, it)\n",
    "    writer.add_scalar('val/emd', emd, it)\n",
    "    writer.flush()\n",
    "\n",
    "    return cd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T17:32:21.672460Z",
     "end_time": "2023-06-17T17:32:21.673454Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def validate_inspect(it):\n",
    "    sum_n = 0\n",
    "    sum_chamfer = 0\n",
    "    # 遍历验证集中的每个批次\n",
    "    for i, batch in enumerate(tqdm(val_loader, desc='Inspect')):\n",
    "        x = batch['pointcloud'].to(args.device)\n",
    "        model.eval()\n",
    "        code = model.encode(x)\n",
    "        recons = model.decode(code, x.size(1), flexibility=args.flexibility).detach()\n",
    "        # 更新计数器\n",
    "        sum_n += x.size(0)\n",
    "        # 如果达到指定的检查批次数目，则退出循环，只检查指定的批次数\n",
    "        if i >= args.num_inspect_batches:\n",
    "            break   # Inspect only 5 batch\n",
    "    # 将重建的点云可视化，并使用 TensorBoard 进行记录\n",
    "    writer.add_mesh('val/pointcloud', recons[:args.num_inspect_pointclouds], global_step=it)\n",
    "    writer.flush()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T17:32:42.620439Z",
     "end_time": "2023-06-17T17:32:42.624390Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-06-17 16:48:59,238::train::INFO] Start training...\n",
      "[2023-06-17 16:49:02,661::train::INFO] [Train] Iter 0001 | Loss 0.592925 | Grad 1.6427 \n",
      "[2023-06-17 16:49:03,395::train::INFO] [Train] Iter 0002 | Loss 0.584033 | Grad 2.1974 \n",
      "[2023-06-17 16:49:04,379::train::INFO] [Train] Iter 0003 | Loss 0.608077 | Grad 1.0924 \n",
      "[2023-06-17 16:49:05,287::train::INFO] [Train] Iter 0004 | Loss 0.558488 | Grad 0.5156 \n",
      "[2023-06-17 16:49:06,447::train::INFO] [Train] Iter 0005 | Loss 0.564405 | Grad 0.3766 \n",
      "[2023-06-17 16:49:07,490::train::INFO] [Train] Iter 0006 | Loss 0.523171 | Grad 0.6145 \n",
      "[2023-06-17 16:49:08,558::train::INFO] [Train] Iter 0007 | Loss 0.673214 | Grad 0.5378 \n",
      "[2023-06-17 16:49:10,619::train::INFO] [Train] Iter 0008 | Loss 0.569181 | Grad 0.3076 \n",
      "[2023-06-17 16:49:12,353::train::INFO] [Train] Iter 0009 | Loss 0.370654 | Grad 0.2336 \n",
      "[2023-06-17 16:49:15,482::train::INFO] [Train] Iter 0010 | Loss 0.540885 | Grad 0.3158 \n",
      "[2023-06-17 16:49:18,425::train::INFO] [Train] Iter 0011 | Loss 0.526445 | Grad 0.3491 \n",
      "[2023-06-17 16:49:20,325::train::INFO] [Train] Iter 0012 | Loss 0.612311 | Grad 0.3106 \n",
      "[2023-06-17 16:49:21,438::train::INFO] [Train] Iter 0013 | Loss 0.387234 | Grad 0.1759 \n",
      "[2023-06-17 16:49:23,033::train::INFO] [Train] Iter 0014 | Loss 0.636383 | Grad 0.1245 \n",
      "[2023-06-17 16:49:24,791::train::INFO] [Train] Iter 0015 | Loss 0.579449 | Grad 0.1720 \n",
      "[2023-06-17 16:49:25,964::train::INFO] [Train] Iter 0016 | Loss 0.451905 | Grad 0.2326 \n",
      "[2023-06-17 16:49:27,237::train::INFO] [Train] Iter 0017 | Loss 0.533006 | Grad 0.2260 \n",
      "[2023-06-17 16:49:28,183::train::INFO] [Train] Iter 0018 | Loss 0.576100 | Grad 0.1629 \n",
      "[2023-06-17 16:49:29,118::train::INFO] [Train] Iter 0019 | Loss 0.566310 | Grad 0.1102 \n",
      "[2023-06-17 16:49:30,057::train::INFO] [Train] Iter 0020 | Loss 0.569300 | Grad 0.1370 \n",
      "[2023-06-17 16:49:31,323::train::INFO] [Train] Iter 0021 | Loss 0.476099 | Grad 0.1816 \n",
      "[2023-06-17 16:49:32,620::train::INFO] [Train] Iter 0022 | Loss 0.565660 | Grad 0.2099 \n",
      "[2023-06-17 16:49:33,853::train::INFO] [Train] Iter 0023 | Loss 0.451570 | Grad 0.1995 \n",
      "[2023-06-17 16:49:35,195::train::INFO] [Train] Iter 0024 | Loss 0.464804 | Grad 0.1774 \n",
      "[2023-06-17 16:49:36,249::train::INFO] [Train] Iter 0025 | Loss 0.542304 | Grad 0.1991 \n",
      "[2023-06-17 16:49:38,419::train::INFO] [Train] Iter 0026 | Loss 0.472628 | Grad 0.2870 \n",
      "[2023-06-17 16:49:39,317::train::INFO] [Train] Iter 0027 | Loss 0.464533 | Grad 0.5090 \n",
      "[2023-06-17 16:49:40,338::train::INFO] [Train] Iter 0028 | Loss 0.452064 | Grad 0.6279 \n",
      "[2023-06-17 16:49:41,334::train::INFO] [Train] Iter 0029 | Loss 0.439301 | Grad 0.3633 \n",
      "[2023-06-17 16:49:42,239::train::INFO] [Train] Iter 0030 | Loss 0.421855 | Grad 0.3259 \n",
      "[2023-06-17 16:49:43,248::train::INFO] [Train] Iter 0031 | Loss 0.414138 | Grad 0.2444 \n",
      "[2023-06-17 16:49:44,300::train::INFO] [Train] Iter 0032 | Loss 0.369743 | Grad 0.3677 \n",
      "[2023-06-17 16:49:45,714::train::INFO] [Train] Iter 0033 | Loss 0.394332 | Grad 0.2209 \n",
      "[2023-06-17 16:49:47,244::train::INFO] [Train] Iter 0034 | Loss 0.473662 | Grad 0.3272 \n",
      "[2023-06-17 16:49:47,964::train::INFO] [Train] Iter 0035 | Loss 0.388527 | Grad 0.2779 \n",
      "[2023-06-17 16:49:48,541::train::INFO] [Train] Iter 0036 | Loss 0.408257 | Grad 0.2316 \n",
      "[2023-06-17 16:49:49,523::train::INFO] [Train] Iter 0037 | Loss 0.402011 | Grad 0.2119 \n",
      "[2023-06-17 16:49:51,636::train::INFO] [Train] Iter 0038 | Loss 0.397184 | Grad 0.2457 \n",
      "[2023-06-17 16:49:52,815::train::INFO] [Train] Iter 0039 | Loss 0.387369 | Grad 0.2142 \n",
      "[2023-06-17 16:49:54,359::train::INFO] [Train] Iter 0040 | Loss 0.402983 | Grad 0.2089 \n",
      "[2023-06-17 16:49:56,020::train::INFO] [Train] Iter 0041 | Loss 0.401135 | Grad 0.1599 \n",
      "[2023-06-17 16:49:57,296::train::INFO] [Train] Iter 0042 | Loss 0.383848 | Grad 0.1996 \n",
      "[2023-06-17 16:49:57,961::train::INFO] [Train] Iter 0043 | Loss 0.398669 | Grad 0.1984 \n",
      "[2023-06-17 16:49:58,810::train::INFO] [Train] Iter 0044 | Loss 0.392942 | Grad 0.1930 \n",
      "[2023-06-17 16:49:59,499::train::INFO] [Train] Iter 0045 | Loss 0.418424 | Grad 0.1604 \n",
      "[2023-06-17 16:50:00,136::train::INFO] [Train] Iter 0046 | Loss 0.452959 | Grad 0.2581 \n",
      "[2023-06-17 16:50:00,736::train::INFO] [Train] Iter 0047 | Loss 0.404350 | Grad 0.2504 \n",
      "[2023-06-17 16:50:01,506::train::INFO] [Train] Iter 0048 | Loss 0.361022 | Grad 0.2549 \n",
      "[2023-06-17 16:50:02,238::train::INFO] [Train] Iter 0049 | Loss 0.438935 | Grad 0.2022 \n",
      "[2023-06-17 16:50:03,236::train::INFO] [Train] Iter 0050 | Loss 0.363472 | Grad 0.1424 \n",
      "[2023-06-17 16:50:04,856::train::INFO] [Train] Iter 0051 | Loss 0.357326 | Grad 0.1457 \n",
      "[2023-06-17 16:50:06,061::train::INFO] [Train] Iter 0052 | Loss 0.417565 | Grad 0.2136 \n",
      "[2023-06-17 16:50:07,587::train::INFO] [Train] Iter 0053 | Loss 0.413742 | Grad 0.1416 \n",
      "[2023-06-17 16:50:08,737::train::INFO] [Train] Iter 0054 | Loss 0.341011 | Grad 0.1973 \n",
      "[2023-06-17 16:50:09,369::train::INFO] [Train] Iter 0055 | Loss 0.441487 | Grad 0.1229 \n",
      "[2023-06-17 16:50:10,402::train::INFO] [Train] Iter 0056 | Loss 0.374663 | Grad 0.1708 \n",
      "[2023-06-17 16:50:11,436::train::INFO] [Train] Iter 0057 | Loss 0.410237 | Grad 0.1122 \n",
      "[2023-06-17 16:50:13,958::train::INFO] [Train] Iter 0058 | Loss 0.419504 | Grad 0.1237 \n",
      "[2023-06-17 16:50:15,337::train::INFO] [Train] Iter 0059 | Loss 0.357192 | Grad 0.1370 \n",
      "[2023-06-17 16:50:17,293::train::INFO] [Train] Iter 0060 | Loss 0.382722 | Grad 0.1057 \n",
      "[2023-06-17 16:50:19,207::train::INFO] [Train] Iter 0061 | Loss 0.443333 | Grad 0.1862 \n",
      "[2023-06-17 16:50:21,007::train::INFO] [Train] Iter 0062 | Loss 0.388226 | Grad 0.1026 \n",
      "[2023-06-17 16:50:34,889::train::INFO] [Train] Iter 0063 | Loss 0.346855 | Grad 0.1888 \n",
      "[2023-06-17 16:51:05,203::train::INFO] [Train] Iter 0064 | Loss 0.342379 | Grad 0.1397 \n",
      "[2023-06-17 16:51:06,716::train::INFO] [Train] Iter 0065 | Loss 0.370253 | Grad 0.1206 \n",
      "[2023-06-17 16:51:08,255::train::INFO] [Train] Iter 0066 | Loss 0.410915 | Grad 0.1676 \n",
      "[2023-06-17 16:51:09,742::train::INFO] [Train] Iter 0067 | Loss 0.414470 | Grad 0.1498 \n",
      "[2023-06-17 16:51:14,460::train::INFO] [Train] Iter 0068 | Loss 0.442471 | Grad 0.1641 \n",
      "[2023-06-17 16:51:17,012::train::INFO] [Train] Iter 0069 | Loss 0.357851 | Grad 0.2525 \n",
      "[2023-06-17 16:51:19,497::train::INFO] [Train] Iter 0070 | Loss 0.495264 | Grad 0.1983 \n",
      "[2023-06-17 16:51:20,836::train::INFO] [Train] Iter 0071 | Loss 0.416426 | Grad 0.1458 \n",
      "[2023-06-17 16:51:23,401::train::INFO] [Train] Iter 0072 | Loss 0.358344 | Grad 0.2063 \n",
      "[2023-06-17 16:51:25,137::train::INFO] [Train] Iter 0073 | Loss 0.337323 | Grad 0.1924 \n",
      "[2023-06-17 16:51:27,187::train::INFO] [Train] Iter 0074 | Loss 0.430529 | Grad 0.2254 \n",
      "[2023-06-17 16:51:27,958::train::INFO] [Train] Iter 0075 | Loss 0.424461 | Grad 0.1880 \n",
      "[2023-06-17 16:51:29,373::train::INFO] [Train] Iter 0076 | Loss 0.356975 | Grad 0.2013 \n",
      "[2023-06-17 16:51:31,602::train::INFO] [Train] Iter 0077 | Loss 0.361563 | Grad 0.1839 \n",
      "[2023-06-17 16:51:33,997::train::INFO] [Train] Iter 0078 | Loss 0.420756 | Grad 0.2430 \n",
      "[2023-06-17 16:51:36,835::train::INFO] [Train] Iter 0079 | Loss 0.407574 | Grad 0.1699 \n",
      "[2023-06-17 16:51:38,799::train::INFO] [Train] Iter 0080 | Loss 0.331339 | Grad 0.2862 \n",
      "[2023-06-17 16:51:39,600::train::INFO] [Train] Iter 0081 | Loss 0.344647 | Grad 0.1641 \n",
      "[2023-06-17 16:51:40,570::train::INFO] [Train] Iter 0082 | Loss 0.375460 | Grad 0.2034 \n",
      "[2023-06-17 16:51:41,350::train::INFO] [Train] Iter 0083 | Loss 0.339512 | Grad 0.1378 \n",
      "[2023-06-17 16:51:42,087::train::INFO] [Train] Iter 0084 | Loss 0.345440 | Grad 0.1799 \n",
      "[2023-06-17 16:51:42,796::train::INFO] [Train] Iter 0085 | Loss 0.373122 | Grad 0.1436 \n",
      "[2023-06-17 16:51:46,295::train::INFO] [Train] Iter 0086 | Loss 0.362720 | Grad 0.2166 \n",
      "[2023-06-17 16:51:47,249::train::INFO] [Train] Iter 0087 | Loss 0.393280 | Grad 0.1209 \n",
      "[2023-06-17 16:51:47,779::train::INFO] [Train] Iter 0088 | Loss 0.383005 | Grad 0.1822 \n",
      "[2023-06-17 16:51:48,348::train::INFO] [Train] Iter 0089 | Loss 0.387748 | Grad 0.1264 \n",
      "[2023-06-17 16:51:48,880::train::INFO] [Train] Iter 0090 | Loss 0.408345 | Grad 0.1951 \n",
      "[2023-06-17 16:51:49,445::train::INFO] [Train] Iter 0091 | Loss 0.346438 | Grad 0.1948 \n",
      "[2023-06-17 16:51:50,105::train::INFO] [Train] Iter 0092 | Loss 0.393203 | Grad 0.1414 \n",
      "[2023-06-17 16:51:50,689::train::INFO] [Train] Iter 0093 | Loss 0.367202 | Grad 0.1914 \n",
      "[2023-06-17 16:51:51,518::train::INFO] [Train] Iter 0094 | Loss 0.470095 | Grad 0.2067 \n",
      "[2023-06-17 16:51:52,438::train::INFO] [Train] Iter 0095 | Loss 0.389789 | Grad 0.1337 \n",
      "[2023-06-17 16:51:53,046::train::INFO] [Train] Iter 0096 | Loss 0.339952 | Grad 0.1596 \n",
      "[2023-06-17 16:51:53,532::train::INFO] [Train] Iter 0097 | Loss 0.373235 | Grad 0.1321 \n",
      "[2023-06-17 16:51:54,008::train::INFO] [Train] Iter 0098 | Loss 0.412589 | Grad 0.1039 \n",
      "[2023-06-17 16:51:54,630::train::INFO] [Train] Iter 0099 | Loss 0.378971 | Grad 0.1137 \n",
      "[2023-06-17 16:51:55,301::train::INFO] [Train] Iter 0100 | Loss 0.349580 | Grad 0.1265 \n",
      "[2023-06-17 16:51:55,831::train::INFO] [Train] Iter 0101 | Loss 0.427367 | Grad 0.1422 \n",
      "[2023-06-17 16:51:56,344::train::INFO] [Train] Iter 0102 | Loss 0.400890 | Grad 0.0930 \n",
      "[2023-06-17 16:51:57,141::train::INFO] [Train] Iter 0103 | Loss 0.342233 | Grad 0.1558 \n",
      "[2023-06-17 16:51:57,711::train::INFO] [Train] Iter 0104 | Loss 0.414548 | Grad 0.1309 \n",
      "[2023-06-17 16:51:58,319::train::INFO] [Train] Iter 0105 | Loss 0.388105 | Grad 0.0963 \n",
      "[2023-06-17 16:51:58,797::train::INFO] [Train] Iter 0106 | Loss 0.348733 | Grad 0.0935 \n",
      "[2023-06-17 16:51:59,313::train::INFO] [Train] Iter 0107 | Loss 0.376419 | Grad 0.0991 \n",
      "[2023-06-17 16:51:59,802::train::INFO] [Train] Iter 0108 | Loss 0.385490 | Grad 0.1188 \n",
      "[2023-06-17 16:52:00,261::train::INFO] [Train] Iter 0109 | Loss 0.374102 | Grad 0.0950 \n",
      "[2023-06-17 16:52:00,705::train::INFO] [Train] Iter 0110 | Loss 0.400287 | Grad 0.1266 \n",
      "[2023-06-17 16:52:01,204::train::INFO] [Train] Iter 0111 | Loss 0.374076 | Grad 0.1254 \n",
      "[2023-06-17 16:52:01,679::train::INFO] [Train] Iter 0112 | Loss 0.437089 | Grad 0.1029 \n",
      "[2023-06-17 16:52:02,135::train::INFO] [Train] Iter 0113 | Loss 0.416816 | Grad 0.1464 \n",
      "[2023-06-17 16:52:02,570::train::INFO] [Train] Iter 0114 | Loss 0.363664 | Grad 0.1945 \n",
      "[2023-06-17 16:52:02,998::train::INFO] [Train] Iter 0115 | Loss 0.358796 | Grad 0.1395 \n",
      "[2023-06-17 16:52:03,433::train::INFO] [Train] Iter 0116 | Loss 0.360041 | Grad 0.1029 \n",
      "[2023-06-17 16:52:03,869::train::INFO] [Train] Iter 0117 | Loss 0.381143 | Grad 0.1871 \n",
      "[2023-06-17 16:52:04,307::train::INFO] [Train] Iter 0118 | Loss 0.354090 | Grad 0.1024 \n",
      "[2023-06-17 16:52:04,744::train::INFO] [Train] Iter 0119 | Loss 0.374069 | Grad 0.1414 \n",
      "[2023-06-17 16:52:05,169::train::INFO] [Train] Iter 0120 | Loss 0.427278 | Grad 0.1677 \n",
      "[2023-06-17 16:52:05,609::train::INFO] [Train] Iter 0121 | Loss 0.366388 | Grad 0.1321 \n",
      "[2023-06-17 16:52:06,037::train::INFO] [Train] Iter 0122 | Loss 0.368338 | Grad 0.1334 \n",
      "[2023-06-17 16:52:06,498::train::INFO] [Train] Iter 0123 | Loss 0.359540 | Grad 0.1506 \n",
      "[2023-06-17 16:52:06,936::train::INFO] [Train] Iter 0124 | Loss 0.376874 | Grad 0.1562 \n",
      "[2023-06-17 16:52:07,396::train::INFO] [Train] Iter 0125 | Loss 0.363763 | Grad 0.0973 \n",
      "[2023-06-17 16:52:07,824::train::INFO] [Train] Iter 0126 | Loss 0.334997 | Grad 0.1519 \n",
      "[2023-06-17 16:52:08,255::train::INFO] [Train] Iter 0127 | Loss 0.404802 | Grad 0.1742 \n",
      "[2023-06-17 16:52:08,687::train::INFO] [Train] Iter 0128 | Loss 0.402106 | Grad 0.1475 \n",
      "[2023-06-17 16:52:09,107::train::INFO] [Train] Iter 0129 | Loss 0.410345 | Grad 0.1037 \n",
      "[2023-06-17 16:52:09,553::train::INFO] [Train] Iter 0130 | Loss 0.368878 | Grad 0.1426 \n",
      "[2023-06-17 16:52:09,998::train::INFO] [Train] Iter 0131 | Loss 0.363709 | Grad 0.1406 \n",
      "[2023-06-17 16:52:10,501::train::INFO] [Train] Iter 0132 | Loss 0.376966 | Grad 0.1333 \n",
      "[2023-06-17 16:52:10,907::train::INFO] [Train] Iter 0133 | Loss 0.324339 | Grad 0.1197 \n",
      "[2023-06-17 16:52:11,300::train::INFO] [Train] Iter 0134 | Loss 0.341464 | Grad 0.0775 \n",
      "[2023-06-17 16:52:11,674::train::INFO] [Train] Iter 0135 | Loss 0.340087 | Grad 0.0987 \n",
      "[2023-06-17 16:52:12,176::train::INFO] [Train] Iter 0136 | Loss 0.365460 | Grad 0.1254 \n",
      "[2023-06-17 16:52:12,689::train::INFO] [Train] Iter 0137 | Loss 0.393405 | Grad 0.1477 \n",
      "[2023-06-17 16:52:13,205::train::INFO] [Train] Iter 0138 | Loss 0.381098 | Grad 0.1188 \n",
      "[2023-06-17 16:52:13,667::train::INFO] [Train] Iter 0139 | Loss 0.371824 | Grad 0.1221 \n",
      "[2023-06-17 16:52:14,099::train::INFO] [Train] Iter 0140 | Loss 0.378697 | Grad 0.1881 \n",
      "[2023-06-17 16:52:14,534::train::INFO] [Train] Iter 0141 | Loss 0.391031 | Grad 0.1802 \n",
      "[2023-06-17 16:52:14,970::train::INFO] [Train] Iter 0142 | Loss 0.349719 | Grad 0.1760 \n",
      "[2023-06-17 16:52:15,401::train::INFO] [Train] Iter 0143 | Loss 0.349247 | Grad 0.1256 \n",
      "[2023-06-17 16:52:15,861::train::INFO] [Train] Iter 0144 | Loss 0.358704 | Grad 0.1227 \n",
      "[2023-06-17 16:52:16,326::train::INFO] [Train] Iter 0145 | Loss 0.399638 | Grad 0.1603 \n",
      "[2023-06-17 16:52:16,763::train::INFO] [Train] Iter 0146 | Loss 0.396099 | Grad 0.1791 \n",
      "[2023-06-17 16:52:17,188::train::INFO] [Train] Iter 0147 | Loss 0.378200 | Grad 0.1567 \n",
      "[2023-06-17 16:52:17,605::train::INFO] [Train] Iter 0148 | Loss 0.374120 | Grad 0.1144 \n",
      "[2023-06-17 16:52:18,027::train::INFO] [Train] Iter 0149 | Loss 0.395544 | Grad 0.1588 \n",
      "[2023-06-17 16:52:18,453::train::INFO] [Train] Iter 0150 | Loss 0.389396 | Grad 0.1321 \n",
      "[2023-06-17 16:52:18,867::train::INFO] [Train] Iter 0151 | Loss 0.354560 | Grad 0.1432 \n",
      "[2023-06-17 16:52:19,296::train::INFO] [Train] Iter 0152 | Loss 0.382359 | Grad 0.1194 \n",
      "[2023-06-17 16:52:19,736::train::INFO] [Train] Iter 0153 | Loss 0.380948 | Grad 0.1246 \n",
      "[2023-06-17 16:52:20,164::train::INFO] [Train] Iter 0154 | Loss 0.325673 | Grad 0.1582 \n",
      "[2023-06-17 16:52:20,595::train::INFO] [Train] Iter 0155 | Loss 0.333091 | Grad 0.1306 \n",
      "[2023-06-17 16:52:21,034::train::INFO] [Train] Iter 0156 | Loss 0.379130 | Grad 0.1927 \n",
      "[2023-06-17 16:52:21,483::train::INFO] [Train] Iter 0157 | Loss 0.410459 | Grad 0.1784 \n",
      "[2023-06-17 16:52:21,921::train::INFO] [Train] Iter 0158 | Loss 0.374935 | Grad 0.1463 \n",
      "[2023-06-17 16:52:22,332::train::INFO] [Train] Iter 0159 | Loss 0.342735 | Grad 0.2424 \n",
      "[2023-06-17 16:52:22,764::train::INFO] [Train] Iter 0160 | Loss 0.371002 | Grad 0.1842 \n",
      "[2023-06-17 16:52:23,227::train::INFO] [Train] Iter 0161 | Loss 0.367689 | Grad 0.2229 \n",
      "[2023-06-17 16:52:23,604::train::INFO] [Train] Iter 0162 | Loss 0.373084 | Grad 0.1881 \n",
      "[2023-06-17 16:52:24,034::train::INFO] [Train] Iter 0163 | Loss 0.381757 | Grad 0.1993 \n",
      "[2023-06-17 16:52:24,474::train::INFO] [Train] Iter 0164 | Loss 0.391773 | Grad 0.1681 \n",
      "[2023-06-17 16:52:25,009::train::INFO] [Train] Iter 0165 | Loss 0.336379 | Grad 0.1702 \n",
      "[2023-06-17 16:52:25,493::train::INFO] [Train] Iter 0166 | Loss 0.351861 | Grad 0.1514 \n",
      "[2023-06-17 16:52:25,899::train::INFO] [Train] Iter 0167 | Loss 0.335576 | Grad 0.2185 \n",
      "[2023-06-17 16:52:26,461::train::INFO] [Train] Iter 0168 | Loss 0.373410 | Grad 0.1670 \n",
      "[2023-06-17 16:52:26,996::train::INFO] [Train] Iter 0169 | Loss 0.397792 | Grad 0.1637 \n",
      "[2023-06-17 16:52:27,397::train::INFO] [Train] Iter 0170 | Loss 0.355535 | Grad 0.1805 \n",
      "[2023-06-17 16:52:27,798::train::INFO] [Train] Iter 0171 | Loss 0.358789 | Grad 0.0864 \n",
      "[2023-06-17 16:52:28,197::train::INFO] [Train] Iter 0172 | Loss 0.336207 | Grad 0.1470 \n",
      "[2023-06-17 16:52:28,602::train::INFO] [Train] Iter 0173 | Loss 0.340676 | Grad 0.1427 \n",
      "[2023-06-17 16:52:28,997::train::INFO] [Train] Iter 0174 | Loss 0.348007 | Grad 0.1735 \n",
      "[2023-06-17 16:52:29,393::train::INFO] [Train] Iter 0175 | Loss 0.377661 | Grad 0.1011 \n",
      "[2023-06-17 16:52:29,794::train::INFO] [Train] Iter 0176 | Loss 0.377767 | Grad 0.1591 \n",
      "[2023-06-17 16:52:30,194::train::INFO] [Train] Iter 0177 | Loss 0.350046 | Grad 0.1232 \n",
      "[2023-06-17 16:52:30,593::train::INFO] [Train] Iter 0178 | Loss 0.371310 | Grad 0.1185 \n",
      "[2023-06-17 16:52:30,996::train::INFO] [Train] Iter 0179 | Loss 0.352140 | Grad 0.1207 \n",
      "[2023-06-17 16:52:31,394::train::INFO] [Train] Iter 0180 | Loss 0.386264 | Grad 0.1627 \n",
      "[2023-06-17 16:52:31,795::train::INFO] [Train] Iter 0181 | Loss 0.374072 | Grad 0.1174 \n",
      "[2023-06-17 16:52:32,198::train::INFO] [Train] Iter 0182 | Loss 0.328563 | Grad 0.1402 \n",
      "[2023-06-17 16:52:32,596::train::INFO] [Train] Iter 0183 | Loss 0.381951 | Grad 0.1528 \n",
      "[2023-06-17 16:52:32,997::train::INFO] [Train] Iter 0184 | Loss 0.352754 | Grad 0.1264 \n",
      "[2023-06-17 16:52:33,398::train::INFO] [Train] Iter 0185 | Loss 0.352013 | Grad 0.1325 \n",
      "[2023-06-17 16:52:33,795::train::INFO] [Train] Iter 0186 | Loss 0.379592 | Grad 0.1601 \n",
      "[2023-06-17 16:52:34,197::train::INFO] [Train] Iter 0187 | Loss 0.344290 | Grad 0.1124 \n",
      "[2023-06-17 16:52:34,619::train::INFO] [Train] Iter 0188 | Loss 0.357105 | Grad 0.1706 \n",
      "[2023-06-17 16:52:34,960::train::INFO] [Train] Iter 0189 | Loss 0.366750 | Grad 0.0813 \n",
      "[2023-06-17 16:52:35,361::train::INFO] [Train] Iter 0190 | Loss 0.340980 | Grad 0.1460 \n",
      "[2023-06-17 16:52:35,762::train::INFO] [Train] Iter 0191 | Loss 0.353992 | Grad 0.1013 \n",
      "[2023-06-17 16:52:36,170::train::INFO] [Train] Iter 0192 | Loss 0.339882 | Grad 0.1485 \n",
      "[2023-06-17 16:52:36,595::train::INFO] [Train] Iter 0193 | Loss 0.322132 | Grad 0.1115 \n",
      "[2023-06-17 16:52:37,004::train::INFO] [Train] Iter 0194 | Loss 0.406782 | Grad 0.1238 \n",
      "[2023-06-17 16:52:37,428::train::INFO] [Train] Iter 0195 | Loss 0.345123 | Grad 0.1298 \n",
      "[2023-06-17 16:52:37,835::train::INFO] [Train] Iter 0196 | Loss 0.341815 | Grad 0.1110 \n",
      "[2023-06-17 16:52:38,248::train::INFO] [Train] Iter 0197 | Loss 0.349164 | Grad 0.0914 \n",
      "[2023-06-17 16:52:38,658::train::INFO] [Train] Iter 0198 | Loss 0.385611 | Grad 0.1383 \n",
      "[2023-06-17 16:52:39,068::train::INFO] [Train] Iter 0199 | Loss 0.424345 | Grad 0.1865 \n",
      "[2023-06-17 16:52:39,467::train::INFO] [Train] Iter 0200 | Loss 0.376417 | Grad 0.1328 \n",
      "[2023-06-17 16:52:39,892::train::INFO] [Train] Iter 0201 | Loss 0.312964 | Grad 0.2462 \n",
      "[2023-06-17 16:52:40,300::train::INFO] [Train] Iter 0202 | Loss 0.357318 | Grad 0.2187 \n",
      "[2023-06-17 16:52:40,702::train::INFO] [Train] Iter 0203 | Loss 0.383011 | Grad 0.1363 \n",
      "[2023-06-17 16:52:41,122::train::INFO] [Train] Iter 0204 | Loss 0.397548 | Grad 0.2242 \n",
      "[2023-06-17 16:52:41,532::train::INFO] [Train] Iter 0205 | Loss 0.426491 | Grad 0.1808 \n",
      "[2023-06-17 16:52:41,934::train::INFO] [Train] Iter 0206 | Loss 0.373326 | Grad 0.1604 \n",
      "[2023-06-17 16:52:42,337::train::INFO] [Train] Iter 0207 | Loss 0.401492 | Grad 0.1538 \n",
      "[2023-06-17 16:52:42,750::train::INFO] [Train] Iter 0208 | Loss 0.362387 | Grad 0.2510 \n",
      "[2023-06-17 16:52:43,165::train::INFO] [Train] Iter 0209 | Loss 0.392432 | Grad 0.1231 \n",
      "[2023-06-17 16:52:43,660::train::INFO] [Train] Iter 0210 | Loss 0.366259 | Grad 0.1289 \n",
      "[2023-06-17 16:52:44,225::train::INFO] [Train] Iter 0211 | Loss 0.386070 | Grad 0.1717 \n",
      "[2023-06-17 16:52:44,695::train::INFO] [Train] Iter 0212 | Loss 0.379020 | Grad 0.1517 \n",
      "[2023-06-17 16:52:45,150::train::INFO] [Train] Iter 0213 | Loss 0.308144 | Grad 0.1602 \n",
      "[2023-06-17 16:52:45,591::train::INFO] [Train] Iter 0214 | Loss 0.365771 | Grad 0.2373 \n",
      "[2023-06-17 16:52:46,031::train::INFO] [Train] Iter 0215 | Loss 0.368919 | Grad 0.0996 \n",
      "[2023-06-17 16:52:46,415::train::INFO] [Train] Iter 0216 | Loss 0.315338 | Grad 0.2466 \n",
      "[2023-06-17 16:52:46,846::train::INFO] [Train] Iter 0217 | Loss 0.389257 | Grad 0.1830 \n",
      "[2023-06-17 16:52:47,274::train::INFO] [Train] Iter 0218 | Loss 0.335067 | Grad 0.1497 \n",
      "[2023-06-17 16:52:47,691::train::INFO] [Train] Iter 0219 | Loss 0.325527 | Grad 0.0890 \n",
      "[2023-06-17 16:52:48,107::train::INFO] [Train] Iter 0220 | Loss 0.342443 | Grad 0.1385 \n",
      "[2023-06-17 16:52:48,533::train::INFO] [Train] Iter 0221 | Loss 0.377207 | Grad 0.0904 \n",
      "[2023-06-17 16:52:48,975::train::INFO] [Train] Iter 0222 | Loss 0.356020 | Grad 0.1140 \n",
      "[2023-06-17 16:52:49,395::train::INFO] [Train] Iter 0223 | Loss 0.421671 | Grad 0.1034 \n",
      "[2023-06-17 16:52:49,811::train::INFO] [Train] Iter 0224 | Loss 0.375650 | Grad 0.1538 \n",
      "[2023-06-17 16:52:50,234::train::INFO] [Train] Iter 0225 | Loss 0.394416 | Grad 0.0821 \n",
      "[2023-06-17 16:52:50,668::train::INFO] [Train] Iter 0226 | Loss 0.332454 | Grad 0.1578 \n",
      "[2023-06-17 16:52:51,138::train::INFO] [Train] Iter 0227 | Loss 0.379138 | Grad 0.1016 \n",
      "[2023-06-17 16:52:51,573::train::INFO] [Train] Iter 0228 | Loss 0.379295 | Grad 0.1789 \n",
      "[2023-06-17 16:52:52,259::train::INFO] [Train] Iter 0229 | Loss 0.358162 | Grad 0.1035 \n",
      "[2023-06-17 16:52:52,847::train::INFO] [Train] Iter 0230 | Loss 0.362776 | Grad 0.1522 \n",
      "[2023-06-17 16:52:53,440::train::INFO] [Train] Iter 0231 | Loss 0.354931 | Grad 0.0823 \n",
      "[2023-06-17 16:52:54,125::train::INFO] [Train] Iter 0232 | Loss 0.374153 | Grad 0.1346 \n",
      "[2023-06-17 16:52:54,792::train::INFO] [Train] Iter 0233 | Loss 0.367331 | Grad 0.0607 \n",
      "[2023-06-17 16:52:55,437::train::INFO] [Train] Iter 0234 | Loss 0.356581 | Grad 0.1049 \n",
      "[2023-06-17 16:52:56,142::train::INFO] [Train] Iter 0235 | Loss 0.371093 | Grad 0.0871 \n",
      "[2023-06-17 16:52:56,799::train::INFO] [Train] Iter 0236 | Loss 0.377807 | Grad 0.1382 \n",
      "[2023-06-17 16:52:57,569::train::INFO] [Train] Iter 0237 | Loss 0.347901 | Grad 0.0758 \n",
      "[2023-06-17 16:52:58,156::train::INFO] [Train] Iter 0238 | Loss 0.408609 | Grad 0.1816 \n",
      "[2023-06-17 16:52:58,813::train::INFO] [Train] Iter 0239 | Loss 0.380575 | Grad 0.1027 \n",
      "[2023-06-17 16:52:59,399::train::INFO] [Train] Iter 0240 | Loss 0.347066 | Grad 0.1576 \n",
      "[2023-06-17 16:53:00,040::train::INFO] [Train] Iter 0241 | Loss 0.371614 | Grad 0.1512 \n",
      "[2023-06-17 16:53:00,714::train::INFO] [Train] Iter 0242 | Loss 0.380599 | Grad 0.0929 \n",
      "[2023-06-17 16:53:01,275::train::INFO] [Train] Iter 0243 | Loss 0.372303 | Grad 0.2003 \n",
      "[2023-06-17 16:53:01,917::train::INFO] [Train] Iter 0244 | Loss 0.339060 | Grad 0.1010 \n",
      "[2023-06-17 16:53:02,568::train::INFO] [Train] Iter 0245 | Loss 0.362258 | Grad 0.2096 \n",
      "[2023-06-17 16:53:03,237::train::INFO] [Train] Iter 0246 | Loss 0.351020 | Grad 0.1037 \n",
      "[2023-06-17 16:53:03,870::train::INFO] [Train] Iter 0247 | Loss 0.344968 | Grad 0.1493 \n",
      "[2023-06-17 16:53:04,536::train::INFO] [Train] Iter 0248 | Loss 0.391743 | Grad 0.1063 \n",
      "[2023-06-17 16:53:05,121::train::INFO] [Train] Iter 0249 | Loss 0.374558 | Grad 0.1292 \n",
      "[2023-06-17 16:53:05,774::train::INFO] [Train] Iter 0250 | Loss 0.335228 | Grad 0.1240 \n",
      "[2023-06-17 16:53:06,349::train::INFO] [Train] Iter 0251 | Loss 0.389520 | Grad 0.0962 \n",
      "[2023-06-17 16:53:07,021::train::INFO] [Train] Iter 0252 | Loss 0.426294 | Grad 0.1207 \n",
      "[2023-06-17 16:53:07,656::train::INFO] [Train] Iter 0253 | Loss 0.367578 | Grad 0.0693 \n",
      "[2023-06-17 16:53:08,274::train::INFO] [Train] Iter 0254 | Loss 0.363374 | Grad 0.1104 \n",
      "[2023-06-17 16:53:08,888::train::INFO] [Train] Iter 0255 | Loss 0.305624 | Grad 0.1689 \n",
      "[2023-06-17 16:53:09,490::train::INFO] [Train] Iter 0256 | Loss 0.387784 | Grad 0.1333 \n",
      "[2023-06-17 16:53:10,144::train::INFO] [Train] Iter 0257 | Loss 0.384675 | Grad 0.1398 \n",
      "[2023-06-17 16:53:10,772::train::INFO] [Train] Iter 0258 | Loss 0.363050 | Grad 0.1611 \n",
      "[2023-06-17 16:53:11,448::train::INFO] [Train] Iter 0259 | Loss 0.364729 | Grad 0.1546 \n",
      "[2023-06-17 16:53:12,053::train::INFO] [Train] Iter 0260 | Loss 0.388102 | Grad 0.1678 \n",
      "[2023-06-17 16:53:12,755::train::INFO] [Train] Iter 0261 | Loss 0.337077 | Grad 0.2176 \n",
      "[2023-06-17 16:53:13,369::train::INFO] [Train] Iter 0262 | Loss 0.333120 | Grad 0.0956 \n",
      "[2023-06-17 16:53:13,982::train::INFO] [Train] Iter 0263 | Loss 0.361276 | Grad 0.2070 \n",
      "[2023-06-17 16:53:14,625::train::INFO] [Train] Iter 0264 | Loss 0.374194 | Grad 0.1454 \n",
      "[2023-06-17 16:53:15,274::train::INFO] [Train] Iter 0265 | Loss 0.326297 | Grad 0.2535 \n",
      "[2023-06-17 16:53:15,914::train::INFO] [Train] Iter 0266 | Loss 0.376147 | Grad 0.1428 \n",
      "[2023-06-17 16:53:16,581::train::INFO] [Train] Iter 0267 | Loss 0.335947 | Grad 0.1725 \n",
      "[2023-06-17 16:53:17,170::train::INFO] [Train] Iter 0268 | Loss 0.375870 | Grad 0.1643 \n",
      "[2023-06-17 16:53:17,799::train::INFO] [Train] Iter 0269 | Loss 0.374923 | Grad 0.1825 \n",
      "[2023-06-17 16:53:18,389::train::INFO] [Train] Iter 0270 | Loss 0.330696 | Grad 0.1406 \n",
      "[2023-06-17 16:53:19,091::train::INFO] [Train] Iter 0271 | Loss 0.377886 | Grad 0.1850 \n",
      "[2023-06-17 16:53:19,739::train::INFO] [Train] Iter 0272 | Loss 0.318802 | Grad 0.1765 \n",
      "[2023-06-17 16:53:20,422::train::INFO] [Train] Iter 0273 | Loss 0.393830 | Grad 0.1664 \n",
      "[2023-06-17 16:53:21,070::train::INFO] [Train] Iter 0274 | Loss 0.329308 | Grad 0.0989 \n",
      "[2023-06-17 16:53:21,744::train::INFO] [Train] Iter 0275 | Loss 0.353464 | Grad 0.1028 \n",
      "[2023-06-17 16:53:22,394::train::INFO] [Train] Iter 0276 | Loss 0.322372 | Grad 0.1167 \n",
      "[2023-06-17 16:53:22,979::train::INFO] [Train] Iter 0277 | Loss 0.409196 | Grad 0.1453 \n",
      "[2023-06-17 16:53:23,737::train::INFO] [Train] Iter 0278 | Loss 0.398253 | Grad 0.1354 \n",
      "[2023-06-17 16:53:24,348::train::INFO] [Train] Iter 0279 | Loss 0.388302 | Grad 0.1001 \n",
      "[2023-06-17 16:53:25,031::train::INFO] [Train] Iter 0280 | Loss 0.324704 | Grad 0.1833 \n",
      "[2023-06-17 16:53:25,631::train::INFO] [Train] Iter 0281 | Loss 0.346845 | Grad 0.1430 \n",
      "[2023-06-17 16:53:26,248::train::INFO] [Train] Iter 0282 | Loss 0.344528 | Grad 0.1269 \n",
      "[2023-06-17 16:53:26,834::train::INFO] [Train] Iter 0283 | Loss 0.374803 | Grad 0.1206 \n",
      "[2023-06-17 16:53:27,419::train::INFO] [Train] Iter 0284 | Loss 0.340362 | Grad 0.0800 \n",
      "[2023-06-17 16:53:27,993::train::INFO] [Train] Iter 0285 | Loss 0.345207 | Grad 0.0956 \n",
      "[2023-06-17 16:53:28,700::train::INFO] [Train] Iter 0286 | Loss 0.391759 | Grad 0.0831 \n",
      "[2023-06-17 16:53:29,402::train::INFO] [Train] Iter 0287 | Loss 0.383298 | Grad 0.1225 \n",
      "[2023-06-17 16:53:29,958::train::INFO] [Train] Iter 0288 | Loss 0.354024 | Grad 0.1228 \n",
      "[2023-06-17 16:53:30,591::train::INFO] [Train] Iter 0289 | Loss 0.364817 | Grad 0.1103 \n",
      "[2023-06-17 16:53:31,175::train::INFO] [Train] Iter 0290 | Loss 0.351119 | Grad 0.1769 \n",
      "[2023-06-17 16:53:31,909::train::INFO] [Train] Iter 0291 | Loss 0.385794 | Grad 0.1183 \n",
      "[2023-06-17 16:53:32,592::train::INFO] [Train] Iter 0292 | Loss 0.320553 | Grad 0.1234 \n",
      "[2023-06-17 16:53:33,375::train::INFO] [Train] Iter 0293 | Loss 0.340567 | Grad 0.1247 \n",
      "[2023-06-17 16:53:34,199::train::INFO] [Train] Iter 0294 | Loss 0.374380 | Grad 0.1128 \n",
      "[2023-06-17 16:53:35,056::train::INFO] [Train] Iter 0295 | Loss 0.367578 | Grad 0.1162 \n",
      "[2023-06-17 16:53:35,725::train::INFO] [Train] Iter 0296 | Loss 0.332396 | Grad 0.0972 \n",
      "[2023-06-17 16:53:36,229::train::INFO] [Train] Iter 0297 | Loss 0.311279 | Grad 0.0917 \n",
      "[2023-06-17 16:53:36,817::train::INFO] [Train] Iter 0298 | Loss 0.372211 | Grad 0.1194 \n",
      "[2023-06-17 16:53:37,405::train::INFO] [Train] Iter 0299 | Loss 0.312457 | Grad 0.2906 \n",
      "[2023-06-17 16:53:38,114::train::INFO] [Train] Iter 0300 | Loss 0.358977 | Grad 0.4290 \n",
      "[2023-06-17 16:53:38,806::train::INFO] [Train] Iter 0301 | Loss 0.363942 | Grad 0.0889 \n",
      "[2023-06-17 16:53:39,481::train::INFO] [Train] Iter 0302 | Loss 0.395438 | Grad 0.4620 \n",
      "[2023-06-17 16:53:40,104::train::INFO] [Train] Iter 0303 | Loss 0.354779 | Grad 0.3873 \n",
      "[2023-06-17 16:53:40,734::train::INFO] [Train] Iter 0304 | Loss 0.343609 | Grad 0.2895 \n",
      "[2023-06-17 16:53:41,405::train::INFO] [Train] Iter 0305 | Loss 0.349319 | Grad 0.4630 \n",
      "[2023-06-17 16:53:42,047::train::INFO] [Train] Iter 0306 | Loss 0.411753 | Grad 0.2119 \n",
      "[2023-06-17 16:53:42,719::train::INFO] [Train] Iter 0307 | Loss 0.401316 | Grad 0.2650 \n",
      "[2023-06-17 16:53:43,403::train::INFO] [Train] Iter 0308 | Loss 0.344337 | Grad 0.2050 \n",
      "[2023-06-17 16:53:44,045::train::INFO] [Train] Iter 0309 | Loss 0.324581 | Grad 0.2190 \n",
      "[2023-06-17 16:53:44,670::train::INFO] [Train] Iter 0310 | Loss 0.332051 | Grad 0.1549 \n",
      "[2023-06-17 16:53:45,306::train::INFO] [Train] Iter 0311 | Loss 0.344853 | Grad 0.1740 \n",
      "[2023-06-17 16:53:46,114::train::INFO] [Train] Iter 0312 | Loss 0.335476 | Grad 0.1410 \n",
      "[2023-06-17 16:53:46,957::train::INFO] [Train] Iter 0313 | Loss 0.365082 | Grad 0.1497 \n",
      "[2023-06-17 16:53:47,676::train::INFO] [Train] Iter 0314 | Loss 0.386329 | Grad 0.1785 \n",
      "[2023-06-17 16:53:48,396::train::INFO] [Train] Iter 0315 | Loss 0.349659 | Grad 0.1408 \n",
      "[2023-06-17 16:53:49,098::train::INFO] [Train] Iter 0316 | Loss 0.356909 | Grad 0.1906 \n",
      "[2023-06-17 16:53:49,770::train::INFO] [Train] Iter 0317 | Loss 0.313506 | Grad 0.1548 \n",
      "[2023-06-17 16:53:50,439::train::INFO] [Train] Iter 0318 | Loss 0.348421 | Grad 0.1218 \n"
     ]
    }
   ],
   "source": [
    "# Main loop\n",
    "logger.info('Start training...')\n",
    "try:\n",
    "    # 初始化迭代次数为1\n",
    "    it = 1\n",
    "    # 循环执行训练过程，直到达到最大迭代次数\n",
    "    while it <= args.max_iters:\n",
    "        # 调用train函数进行模型训练，传入当前的迭代次数\n",
    "        train(it)\n",
    "        # 判断是否达到进行验证的时机\n",
    "        if it % args.val_freq == 0 or it == args.max_iters:\n",
    "            with torch.no_grad():\n",
    "                # 计算验证集上的损失\n",
    "                cd_loss = validate_loss(it)\n",
    "                # 对重建的点云进行检查\n",
    "                validate_inspect(it)\n",
    "            # 保存当前的优化器和调度器状态\n",
    "            opt_states = {\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "            }\n",
    "            # 保存模型的状态、参数和优化器状态到检查点文件中\n",
    "            ckpt_mgr.save(model, args, cd_loss, opt_states, step=it)\n",
    "        it += 1\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logger.info('Terminating...')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T16:48:59.242135Z",
     "end_time": "2023-06-17T17:04:26.453055Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "Inspect:   0%|          | 0/19 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db063426e95b49adb9f7f142c58ab84d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(tqdm(val_loader, desc='Inspect')):\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T17:39:25.208328Z",
     "end_time": "2023-06-17T17:39:25.250883Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "gen_pcs = np.load(r'D:\\PycharmProjects\\Replay_continual_learning_2\\pointCloud\\results\\GEN_Ours_car_1687072132\\out.npy')\n",
    "for x in gen_pcs:\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(x)\n",
    "    o3d.visualization.draw_geometries([pcd])\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T00:50:07.308300Z",
     "end_time": "2023-06-19T00:50:11.563454Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from pointCloud.utils.dataset import ShapeNetCore\n",
    "train_d = ShapeNetCore(\n",
    "    path='./data/shapenet.hdf5',\n",
    "    cates=['car'],\n",
    "    split='train',\n",
    "    scale_mode='shape_unit',\n",
    "    transform=None,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T00:50:51.925696Z",
     "end_time": "2023-06-19T00:50:52.353548Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "x = train_d[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T00:50:14.823626Z",
     "end_time": "2023-06-19T00:50:14.831672Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "2986"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_d)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T00:50:54.710807Z",
     "end_time": "2023-06-19T00:50:54.720135Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2048, 3])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['pointcloud'].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T00:52:50.930306Z",
     "end_time": "2023-06-19T00:52:50.939632Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T00:53:36.582184Z",
     "end_time": "2023-06-19T00:53:36.591496Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
